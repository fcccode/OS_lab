## 目录

[TOC]

## 一、实验目的

实验7的实验目的:

1. 完善实验6中的二状态进程模型，实现五状态进程模型，从而使进程可以分工合作，并发运行。
2. 了解派生进程、结束进程、阻塞进程等过程中父、子进程之间的关系和分别进行的操作。
3. 理解原语的概念并实现进程控制原语do_fork(), do_exit(), do_wait(), wakeup, blocked。

## 二、特色简介

本次实验七又是我对原型操作系统的一次巨大改进，有以下功能实现：

1. 实现了完善的基于二级页表的**页式内存管理**，包括完成了分页机制的开启、编写了处理物理内存的页框式分配和回收、页目录和页表初始化、物理地址和虚拟地址的映射和解映射的内核函数。
2. 利用保护模式的**特权级转移**和TSS，实现了在进程中**分离用户栈和内核栈**。
3. 基于页表和特权级转移彻底重写了进程管理机制。从而实现了：
   1. fork系统调用：生成拥有不同虚拟地址空间，资源隔离的，真正意义上的**进程**；
   2. **clone**系统调用：生成共享数据段和代码段地址空间，但拥有不同的栈空间的**线程**；
   3. wait，exit系统调用和sys_do_wait**, **sys_do_sleep**,**wakeup三个内核原语。
   4. 基于以上系统调用和内核原语的五状态进程模型。
4. 重写了exec系统调用，实现了读取加载**ELF**文件。
5. 将shell分离出了内核，变为用户程序，为之后实现多终端打下了基础。

与实验六的代码相比，我共修改了50个文件，新增代码2449行，删除代码1134行。当前**总代码量6032行**。

（统计数据由git和cloc生成，如下图。图中显示了每个文件的改动情况：）

![image-20180524193702193](/var/folders/_1/gkysqkws68gf9rbpywnkjhqm0000gn/T/abnerworks.Typora/image-20180524193702193.png)

![image-20180524193717725](/var/folders/_1/gkysqkws68gf9rbpywnkjhqm0000gn/T/abnerworks.Typora/image-20180524193717725.png)

![image-20180524193904499](/var/folders/_1/gkysqkws68gf9rbpywnkjhqm0000gn/T/abnerworks.Typora/image-20180524193904499.png)

## 三、实验环境

本次实验中我用到了readelf工具研究elf文件结构，用到了strip工具移除elf文件中的调试信息，从而减少elf文件大小，方便本操作系统加载。

其余环境与之前实验大致相同：

IDE：CLion 2018.2

C++编译器：g++ 7.3.0，Target: i386-elf

链接器：ld 2.30

二进制文件分析器：objdump 2.30

符号分析器：nm 2.3.0

主机操作系统：Mac OS 10.12

编辑器：Vim 8.0.1400、VS Code 1.21.0

汇编器：Nasm 2.13.02

虚拟机、调试器：Bochs 2.6.9

版本控制：Git 2.15.1

自动构建：GNU Make 3.8.1

## 四、实验方案

### (1). 页式内存管理

#### 1. 页框式物理内存分配器

该部分实现在./kernel_lib/ram.cpp中

80386硬件支持基于二级页表的页式内存管理，支持从线性地址到物理地址的转换。在开始实现这一点之前，首先我们实现以下两点，把物理内存管理起来

1. 知道有多少物理内存，可以分成多少页
2. 实现分配出去（alloc）和回收（free）

获取物理内存情况是通过在实模式时使用BIOS中断完成的，在实验六中已经实现了。

在得知可用内存字节数后，除以4Kb得到页框数$n$。使用一个堆栈pm_node进行管理，初始化pm_node中压入所有可用的物理内存地址，此时栈大小pm_node_size = $n$。alloc函数可以实现为对于pm_node的出栈操作。free函数则实现为对pm_node的入栈操作。

```c++
uint32_t ram_alloc()
{
    if(pm_node_size == 0)
    {
        debug_printf("No memory\n");
        bochs_break();
        return 0;
    }
    debug_printf("Give you physical memory %x\n", pm_node[pm_node_size - 1]);
    memset((char*)pm_node[pm_node_size - 1], 0, PAGE_SIZE);
    return pm_node[--pm_node_size];
}

void ram_free(uint32_t addr)
{
    if(addr & 0xfff)
    {
        debug_printf("Can not free 0x%x, which is not 4k-aligned\n", addr);
        bochs_break();
        return;
    }
    pm_node[pm_node_size++] = addr;
    if(pm_node_size + 1 > pm_node_cnt)
    {
        debug_printf("All memory is free!");
        bochs_break();
        return;
    }
}
```

#### 2. 初始化内核页表和开启分页

该部分实现在./kernel_lib/page.cpp中。

该部分分以下步骤实现

1. 在数据段定义内核页目录和内核页表

```c++
//页目录
pde_t pgd_kern[PDE_SIZE]
__attribute__((aligned(PAGE_SIZE)));
//页表项
static pte_t pte_kern[PTE_COUNT][PTE_SIZE] __attribute__((aligned(PAGE_SIZE)));
```

注：

- pde和pte分别是page directory entry和page table entry的缩写
- 页目录大小一定是4KB（一页的大小） 
- 页表项数目可以按照需求通过调整PTE_SIZE来调整其大小，目前我设置为PTE_SIZE为1024，每一个页表可以寻址4kb*1024 = 4Mb的空间。也就是让内核拥有完整的4GB地址。
- 必须使用\__attribute__扩展让页表4K对齐

2. 在内核页表中将线性地址直接映射为虚拟地址

我没有像Linux中将内核映射到一个高地址，而是直接将其按照原本的线性地址进行映射。事实上我觉得把内核放在1MB的高地址区已经是安全的了。这样处理起来更加方便，不需要又一次移动内核。

代码实现上是下面的两个循环，先写页目录，然后写页表。

```c++
    for (i = 0, j = 0; i < PTE_COUNT; i++, j++){
        pgd_kern[i] = (uint32_t)pte_kern[j] | PTE_P | PTE_R | PTE_K;
    }

    uint32_t *pte = (uint32_t *)pte_kern;
    for (i = 1; i < PTE_COUNT*PTE_SIZE; i++){
        pte[i] = (i << 12) | PTE_P | PTE_P | PTE_K;
    }
```

3. 加载页表到cr3寄存器

```c++
inline void vmm_switch_pgd(uint32_t pde){
    __asm__ volatile ("mov %0, %%cr3": :"r"(pde));
}
```

4. 启动分页

```c++
static inline void vmm_enable(){
    uint32_t cr0;
    __asm__ volatile ("mov %%cr0, %0" : "=r" (cr0));
    cr0 |= CRO_PG;
    __asm__ volatile ("mov %0, %%cr0" : : "r" (cr0));
}
```

#### 3. 实现与进程相关的页式内存管理函数

在后面实现进程的过程中，要经常进行页表的分配、回收、映射、解映射、切换。因此内核中实现了以下9个函数以支持这些功能：

1. void vmm_map(pde_t *pgdir, uint32_t va, uint32_t pa, uint32_t flags);

   向页目录pgdir指向的页表中写入虚拟内存va和物理内存pa的映射关系，映射以页为单位。它首先查询pgdir，如果有关va的页表还未建立（这发生在新建用户进程时），该函数会首先分配页表。

2. int vmm_get_mapping(pde_t *pgdir, uint32_t va, uint32_t *pa);

   从页目录pgdir指向的页表中查询虚拟地址va对应的物理地址，保存到pa中。

3. void kvm_init(pde_t *pgdir);

   用于初始化一个进程的页表的内核区域。**本实验中我采用的内存分配方式是将3GB以下划为内核区域，用户程序基地址USER_BASE = 0xc0000000。**

4. void uvm_init_fst(pde_t *pgdir, char *init, uint32_t size);

   用以初始化init进程的页表。该函数要特别声明，因为init进程是被编译到内核中，直接移动到用户地址空间，而不是像其他进程是一个进程fork出的子进程调用exec从磁盘加载新进程镜像生成的。这两个过程有实现上的不同。

5. void uvm_switch(PCB *pp);

   修改cr3，将当前页目录切换为进程控制块pp中的pgdir。该过程发生在exec调用执行新用户程序的过程中以及进程调度的过程中，让新的用户程序镜像替换原来的程序。

   该函数还复制修改tss，把其中保存的原来程序的内核堆栈指针改为新的程序的。

6. pde_t *uvm_copy(pte_t *pgdir, uint32_t text_size);

   该函数用于fork调用中。text_size是调用fork的父进程占用的内存空间（不包括堆栈）。该函数使用ram_alloc分配可以容纳text_size字节内存的页，然后将当前进程的镜像拷贝到这些页中，然后建立与当前进程相同的地址映射关系。

7. pde_t *uvm_copy_thread(pte_t *pgdir, uint32_t text_size);

   该函数用于clone调用，新建线程的过程汇总。它仅新建一个页，作为新线程的独立堆栈。除堆栈外，新线程页表直接复制原父进程中的地址映射关系。

8. void uvm_free(pte_t *pgdir);

   使用ram_free清理页目录pgdir和有关的页表。

9. int uvm_alloc(pte_t *pgdir, uint32_t old_sz, uint32_t new_sz);

   该函数用于exec调用，old_sz是原用户程序的镜像大小，new_sz是新用户程序的镜像大小，如果new_sz大于old_sz，那么该函数会分配新的页表，并新建内存以容纳新用户程序。

10. int uvm_load(pte_t *pgdir, uint32_t addr, char* ip, uint32_t off, uint32_t size);

    该函数将用户程序从磁盘缓冲区ip中拷贝到uvm_alloc分配的内存中。

### (2). 进程

#### 1. PCB

当前版本的PCB如下：

```c++
struct PCB {
    uint32_t text_size;       // 进程占用内存空间(数据段和代码段，即镜像大小)
    pde_t* pgdir;             // 进程页目录
    char *kern_stack;         // 内核栈最高地址
    enum procstate state;     // 进程状态
    int pid;                  // 进程ID
    PCB *parent;              // 父进程指针
    int_frame *tf;            // 系统调用或中断时压入的中断帧，细节见上一个实验报告
    context_t *context;       // 进程上下文结构体指针
    void *sleep_event;        // 进程休眠等待的事件
    int killed;               // 是否被kill
    char name[16];            // 进程名
};
```

在当前实现中，进程上下文context_t不再保存所有寄存器，而只根据x86调用规定中只保存“被调用者保存”寄存器（Callee-save registers）。esp没有被显示保存，事实上context指针的值就是esp。

```c++
struct context_t {
    uint32_t edi;
    uint32_t esi;
    uint32_t ebx;
    uint32_t ebp;
    uint32_t eip;
};
```

进程控制块有六种状态，一种是控制块未被使用的状态，另外五种对应进程的创建、就绪、运行、阻塞和终止态。

```c++
enum procstate { 
	P_UNUSED, 		//进程控制块未被使用
	P_USED,			//进程控制块已经被分配，但还未完成初始化，进程处于创建态
	P_RUNNABLE, 	//进程控制块初始化完毕，进程处于就绪态，可以被调度运行
	P_RUNNING,		//进程处于运行态
	P_SLEEPING, 	//进程处于阻塞态
	P_ZOMBIE 		//进程处于终止态，PCB等待被父进程回收
};
```



#### 2. PCB分配和初始化内核栈

PCB是一种资源，目前系统中默认拥有128个PCB，即最大创建128个进程和线程。

获取PCB要调用函数proc_alloc

proc_alloc函数会遍历寻找可用的PCB，如果有，则会进行以下初始化步骤：

1. 为新进程分配内核栈

2. 初始化内核栈，这部分是整个实验中非常巧妙的一部分内容：

   它使得内核栈有以下形式

   <img src="../../../../../../../../var/folders/_1/gkysqkws68gf9rbpywnkjhqm0000gn/T/abnerworks.Typora/image-20180525222205346.png" width="50%" />

   **该形式恰好是一个fork系统调用即将结束时栈中的内容。**

   于是，我们可以将新进程的eip指向一个构造的fork调用的返回阶段函数fork_ret，从该函数返回后，进程就如同从真正的fork调用返回一样，进入了中断返回阶段，而从中断返回会回到特权级为ring3的**用户态**。

   这就是本次试验中实现切换到用户态的原理。

#### 2.创建init进程 

init进程是系统中第一个进程，是所有进程的父进程。

它的代码如下：首先使用int指令调用fork，产生一个子进程，子进程调用exec(192)执行硬盘192号扇区上的shell程序。

```assembly
%include "kernel_lib/pm.inc"
align 4

[bits 32]
[section .text]
[global __init_start]
[global __init_end]

__init_start:
    nop
    ;fork()
    mov ah, 7
    int 0x98   
    cmp eax, 0
    jz child
    jmp $

child:
	;exec(192)
    push 192
    push 0
    mov ah, 8
    int 0x98
    add esp, 8
    jmp $

__init_end:
```

创建该进程的过程为：

1. 调用proc_alloc

2. 调用ram_alloc获取页目录

3. 调用kvm_init初始化页表的内核部分（3GB以下）

4. 调用vmm_map建立有关内核栈的页表项

5. 调用uvm_init_fst初始化init进程的页表。

6. 初始化PCB中储存的寄存器值

   ```c++
       pp->tf->cs = (SEL_UCODE << 3) | 0x3;
       pp->tf->ds = (SEL_UDATA << 3) | 0x3;
       pp->tf->es = pp->tf->ds;
       pp->tf->fs = pp->tf->ds;
       pp->tf->gs = pp->tf->ds;
       pp->tf->ss = pp->tf->ds;
       pp->tf->eflags = FLAG_IF;
       pp->tf->user_esp = USER_TEXT_BASE;
       pp->tf->eip = USER_TEXT_BASE;
   ```

7. 设置进程名为init、状态为P_RUNNABLE

### (3). 进程管理

#### 1. 进程调度器

调度器的作用是寻找并切换到一个就绪态的进程。

在手动构造出init进程后，就可以开启调度器了。

调度器的代码如下：

```c++
void scheduler(){
    PCB *pp;

    debug_puts("scheduler: start\n");

    for (;;){
        for (pp = &ptable[0]; pp < &ptable[MAX_PROC]; pp++){
            asm volatile("cli");
            if (pp->state != P_RUNNABLE){
                continue;
            }

            debug_printf("scheduler: proc `%s`(PID: %d) will run\n", pp->name, pp->pid);

            uvm_switch(pp);
            pp->state = P_RUNNING;

            current_proc = pp;
            debug_puts(">>>> context switch\n");
            sys_context_switch(&cpu_context, pp->context);
            debug_printf("<<<< return form proc `%s`(PID: %d)\n", pp->name, pp->pid);
            asm volatile("sti");
        }
    }
}
```

可见它的工作流程是：

1. 它遍历所有进程控制块，找到控制块序列中最后一个状态为就绪态的进程。
2. 调用uvm_switch加载该进程的页表和内核栈指针
3. 调用sys_context_switch，保存调度器的上下文指针到cpu_context中，并切换到新的进程上下文

sys_context_switch的函数原型为

```c++
extern "C" void sys_context_switch(context_t** p_p_context, context_t* p_context);
```

注意一个进程控制块中并不保存上下文结构体的实体，保存的**就只是**上下文结构体的指针。因此第一个参数是指向原来进程控制块的上下文结构体指针的指针，（C语言通过传指针才能修改一个变量的值），第二个参数是指向要被调度的进程控制块的上下文结构体指针。

实现是汇编代码，代码如下：

```asm
[global sys_context_switch]
sys_context_switch:
    mov eax, [esp + 4]  ; old
    mov edx, [esp + 8]  ; new

    ; eip has been save when call context_switch
    push ebp
    push ebx
    push esi
    push edi

    ; switch stack
    mov [eax], esp      ; save esp
    mov esp, edx

    pop edi
    pop esi
    pop ebx
    pop ebp
    ret
```

在sys_context_switch完成后，eip被切换为被调度的进程的上下文中保存的eip，新的进程开始运行了。

#### 2. 进程切换

啊下图显示了一个时钟中断引发的进程A切换到进程B的过程![image-20180526112818581](../../../../../../../../var/folders/_1/gkysqkws68gf9rbpywnkjhqm0000gn/T/abnerworks.Typora/image-20180526112818581.png)

时钟中断中调动sched函数，该函数代码如下：

```c++
void sched(){
    if (current_proc == nullptr) return;
    if (current_proc->state == P_RUNNABLE)
        debug_puts("sched: no runable\n");

    if (current_proc->state == P_RUNNING){
        current_proc->state = P_RUNNABLE;
    }
    sys_context_switch(&current_proc->context, cpu_context);
}
```

可见，正常情况下，它首先将当前进程状态改为就绪态。然后执行contex_switch，换回scheduler的上下文（cpu_context），于是就回到了scheduler的调度循环中，于是会切换到下一个进程。

#### 2. fork系统调用

fork系统调用由sys_do_fork实现。

```c++
int sys_do_fork(){
    int i;
    PCB *child;

    debug_printf("fork: fork `%s`\n", current_proc->name);

    if ((child = proc_alloc()) == 0){
        return -1;
    }

    debug_puts("fork: copying memory...\n");

    child->pgdir = uvm_copy(current_proc->pgdir, current_proc->text_size);

    if (child->pgdir == 0){
        debug_puts("fork:");
        ram_free((uint32_t)child->kern_stack);
        child->kern_stack = 0;
        child->state = P_UNUSED;
        return -1;
    }

    debug_puts("fork: copying attrib...\n");
    child->text_size = current_proc->text_size;
    child->parent = current_proc;
    *(child->tf) = *(current_proc->tf); // return form same address

    child->tf->eax = 0;
    child->cwd = current_proc->cwd;
    strcpy(child->name, current_proc->name);

    child->state = P_RUNNABLE;

    debug_puts("fork: done\n");
    return child->pid;
}
```

可见，它的工作流程是：

1. 调用proc_alloc为子进程分配新的进程控制块并设置页表内核区
2. 调用uvm_copy拷贝父进程镜像并设置页表
3. 拷贝父进程的上下文和中断帧等信息
4. 将子进程中断帧中的eax寄存器的值设置为0，这样子进程中fork函数的返回值就是0了。
5. 返回子进程pid给调用者（父进程）

#### 3. clone系统调用 

本操作系统设计中使用clone系统调用线程新建线程。

Unix系统中是没有这一个调用的。Linux系统中有这一个调用，它根据传入的参数不同决定是新建进程还是线程；事实上Linux系统中fork函数是用clone实现的。本操作系统中fork已经真实地实现了进程，因此就借用clone这个名字，用于实现线程。

它与fork代码基本相同，唯一不同的就是它不调用uvm_copy，而是调用uvm_copy_thread，仅仅建立独立的堆栈，并通过设置相同的页表映射共享父进程的数据段和代码段。

#### 4. wakeup内核原语

其函数原型为`void wakeup(void *sleep_event)`。用于将因sleep_event而阻塞的进程设为就绪态。

```c++
void wakeup(void *sleep_event){
    PCB *pp;

    for (pp = ptable; pp < &ptable[MAX_PROC]; pp++){
        if (pp->state == P_SLEEPING && pp->sleep_event == sleep_event){
            pp->state = P_RUNNABLE;
        }
    }
}
```

#### 5. sys_do_sleep内核原语

其函数原型为`void sys_do_sleep(void *sleep_event)`。用于实现进程阻塞。

其代码如下：

```c++
void sys_do_sleep(void *sleep_event){
    if(current_proc == nullptr)
        debug_puts("sleep: no proc\n");

    debug_printf("sleep: proc `%s`(PID: %d) is going to sleep...\n", current_proc->name, current_proc->pid);
    asm volatile("cli");
    current_proc->sleep_event = sleep_event;
    current_proc->state = P_SLEEPING;
    asm volatile("sti");

    sched();

    // wake up
    current_proc->sleep_event = nullptr;

    debug_printf("sleep: proc `%s`(PID: %d)  wakeup...\n", current_proc->name, current_proc->pid);

    // yes, we call pic_init again... :(
    pic_init();
}
```

可见，它的工作流程是：

1. 将当前进程A设为为阻塞态，"阻塞原因"为sleep_event。
2. 调用sched切换到新的进程
3. 当进程A被wakeup原语设为就绪态并再次被调度时，会回到sys_do_sleep中第14行继续执行。

#### 6. wait系统调用

该函数用于实现进程间通信，父进程等待子进程结束。它由内核原语sys_do_wait实现。

```c++
int sys_do_wait(){
    int havekids, pid;
    PCB* pp;

    debug_puts("wait: waiting...\n");
    for (;;)
    {
        havekids = 0;
        for (pp = ptable; pp <= &ptable[MAX_PROC]; pp++)
        {
            if (pp->parent != current_proc)
            {
                continue;
            }

            havekids = 1;

            if (pp->state == P_ZOMBIE)
            {
                debug_printf("wait: recycle proc `%s`(PID: %d)\n", pp->name, pp->pid);
                // can be clear
                pid = pp->pid;

                /* free mem */
                ram_free((uint32_t)pp->kern_stack);
                pp->kern_stack = 0;
                uvm_free(pp->pgdir);

                pp->state = P_UNUSED;
                pp->pid = 0;
                pp->parent = 0;
                pp->name[0] = 0;
                pp->killed = 0;

                return pid;
            }
        }

        if (!havekids || current_proc->killed)
        {
            return -1;
        }

        // wait for chidren to exit
        sys_do_sleep(current_proc);
    }
}
```

可见其工作流程是遍历所有进程，直到：

1. 找到一个子进程，子进程尚未结束（不处于P_ZOMBIE态），则调用sys_do_sleep休眠等待
2. 找到一个子进程，子进程处于P_ZOMBIE态，则回收子进程的PCB和占用的内存，等待结束，返回子进程的pid
3. 没有找到子进程，或者父进程自己在等待过程中被kill了，则直接返回-1

#### 7. exit系统调用

该函数用于结束一个进程。它由内核原语sys_do_exit实现。

```c++
void sys_do_exit(){
    PCB *pp;
    int fd;

    if(current_proc == initproc)
        debug_puts("exit: initproc can no exit\n");

    debug_puts("exit: closing opening file\n");

    current_proc->cwd = 0;

    asm volatile("cli");
    //wakeup(proc->parent);

    debug_puts("exit: collecting subprocess\n");
    for (pp = ptable; pp < &current_proc[MAX_PROC]; pp++){
        if (pp->parent == current_proc){
            pp->parent = current_proc->parent;
            if (pp->state == P_ZOMBIE){
                wakeup(current_proc->parent);
            }
        }
    }
    wakeup(current_proc->parent);

    debug_puts("exit: ZOMBIE\n");
    current_proc->state = P_ZOMBIE;
    asm volatile("sti");


    sched();
    debug_puts("exit: return form sched");
    bochs_break();
}
```

可见其工作流程是：

1. 首先检查自己是不是有子进程，如果有，则将自进程的父进程设为自己的父进程。

   注意，这和实际的Unix/Linux系统实现均不同，实际的Unix/Linux系统中，这些子进程父进程会被设为init进程。

2. 将解除当前进程的父进程可能的阻塞态

3. 将当前进程设为终止态

### (4). 执行ELF格式的用户程序

#### 1.用户程序生命周期

下图显示了操作系统中一个用户程序的生命周期。

1. 父进程A调用fork产生一个子进程
2. 子进程B调用exec，从磁盘加载用户程序的镜像，通过加载新的页目录，把自己替换为用户程序
3. 用户程序作为子进程B运行，父进程A等待它结束
4. 用户程序执行到exit，子进程B进入终止态
5. 父进程回收子进程B的PCB和占用的内存

![image-20180526140636006](../../../../../../../../var/folders/_1/gkysqkws68gf9rbpywnkjhqm0000gn/T/abnerworks.Typora/image-20180526140636006.png)

fork、wait、exit的实现在上面已经介绍过了，下面介绍exec的实现。

#### 2. ELF 文件结构 

用户程序在磁盘上以ELF格式存储，ELF文件有一个ELF头部，结构为：

```c++
struct elf32hdr {
    uint32_t magic;   // ELF文件标志
    char elf[12];
    uint16_t type;
    uint16_t machine;
    uint32_t version;
    uint32_t entry;   // 程序入口
    uint32_t phoff;   // 程序头偏移量
    uint32_t shoff;
    uint32_t flags;
    uint16_t ehsize;
    uint16_t phentsize;
    uint16_t phnum;   // 程序头数量
    uint16_t shentsize;
    uint16_t shnum;
    uint16_t shstrndx;
};
```

加载ELF文件时主要用到上面注释过的四项

- uint32_t magic，是一个常数0x464c457f，用以检查要加载的文件是否是一个ELF文件
- uint32_t entry，指定程序入口地址
- uint32_t phoff,  指定程序头表在文件中的位置
- uint16_t phnum，指定程序头数量

程序头表（program header table）是一个程序头（program header）数组。

每个程序头描述了用户程序的一个段，其定义如下：

```c++
struct proghdr{
    uint32_t type;	//段类型
    uint32_t off;	//段偏移量
    uint32_t vaddr;	//物理地址
    uint32_t paddr;	//虚拟地址
    uint32_t filesz;//段在文件中占用的大小
    uint32_t memsz;	//段加载到内存中后占用的大小
    uint32_t flag;	//标志位
    uint32_t align;	//内存对齐情况
};
```

目前本操作系统不支持动态加载，只支持type == 1的静态可加载的段。

#### 3.exec系统调用

exec从磁盘加载用户程序的镜像，并加载新的页目录，把调用者替换为用户程序。

它由sys_do_exec内核原语实现，函数原型是int sys_do_exec(uint32_t n)。n是要加载的程序在磁盘上的起始扇区号。其代码较长，单独列在proc/exec.cpp中，这里只简述其工作流程：

1. 获取并初始化新的页目录。
2. 首先调用sys_read_hard_disk从磁盘n号扇区读取用户程序镜像到磁盘缓存binary_image_buf中，在目前尚未实现文件系统的情况下，简单地对任何程序都加载40个扇区（20kb）。
3. 读取EFL头部信息，检查是否是合法的ELF文件。
4. 获取ELF程序头表偏移量，遍历每一个程序头。
5. 根据程序头中的信息，调用uvm_alloc为每一个段分配新的页表，并新建内存以容纳该段。
6. 调用uvm_load将该段从binary_image_buf中拷贝到uvm_alloc分配的内存中。
7. 完成所有段的加载后，在用户栈中压入命令行参数（目前尚未在shell中写对应的该功能的支持，因此只是压入空的值占位）
8. 保存原来进程的页目录，修改原进程PCB中的页目录指针、进程大小、eip、用户栈esp，最后调用uvm_switch切换内核栈和页目录，就完成了新用户程序的加载。此时可以释放掉原来进程的页目录。

## 五、实验结果

### 1.测试实验课PPT中的测试程序

程序代码如下：（usr/usr1.cpp）

PPT中的实验要求是“由父进程生成一个字符串，交给子进程统计其中字母的个数，然后在父进程中输出这一统计结果。 ”

PPT中的原型系统实现的进程实际上相当于是我这里的共享全局变量在内的资源的线程，因此我这里用的是clone系统调用生成的线程。

```c++
char str[80] = "129djwqhdsajd128dw9i39ie93i8494urjoiew98kdkd";
int LetterNr = 0;
void CountLetter()
{
    for(int i = 0; i < 80; ++i)
    {
        if(isalpha(str[i]))
            ++LetterNr;
    }
}
extern "C" void main() {
    int pid;
    pid = clone();
    if (pid == -1)
        printf("error in fork !");
    if (pid) {
        wait();
        printf("LetterNr = %d\n", LetterNr);
    }
    else {
        CountLetter();
        exit();
    }
}
```

运行结果如下图，在shell中输入usr1后程序开始运行，shell显示自己新建了用户程序，pid为3。

接下来就是用户程序的输出：LetterNr = 27。可见它正确执行并统计出了字母数量，实验成功。

![image-20180526152813412](../../../../../../../../var/folders/_1/gkysqkws68gf9rbpywnkjhqm0000gn/T/abnerworks.Typora/image-20180526152813412.png)

#### 2.观察init进程的创建过程和页分配情况

使用上次实验中我发现的通过e9端口向物理机的bochs终端输出调试信息的方法，我编写了debug_printf，并在内核各个函数中做了日志，格式为函数名：日志信息，仅输出到bochs终端的是红色，同时输出到虚拟机界面的是蓝色。

从而我记录了init进程创建的过程，如下图：

![image-20180526153019888](../../../../../../../../var/folders/_1/gkysqkws68gf9rbpywnkjhqm0000gn/T/abnerworks.Typora/image-20180526153958129.png)

首先是物理内存检测的过程。我分配给bochs的物理内存为32MB，内核占用了4732kb，剩下26940Kb内存被划分为6735个页框。

新建init的过程中，起始地址为0x1fef000的最高可用物理内存页作为第一个被分配出去的页，用作了它的内核栈。起始于0x1fee000的页用作了它的页目录。起始于0x1fed000的页用作了用户栈，vmm_map试图在页表中将其映射到0xc0000000，但发现页目录中还没有有关0xc0000000的页表项，因此首先获取起始于0x1fec000的页作为页表。由于init进程大小为28字节，因此仅分配了从0x1feb000开始的一个页用于存储init的数据和代码段，并映射到0xc0001000

## 六、实验总结

本次实验是7次实验中，**代码量最大**，**理论难度最大**的一个。

首先是为了实现分页，我花了很长时间研究保护模式中各种概念，包括:

- 特权级相关的：DPL、CPL、RPL，一致代码段段，非一致代码段；
- 上下文切换相关的：TSS、TR寄存器；
- 分页相关的：页目录、页表、页表项结构、cr0、cr3寄存器；

这部分是十分底层的，代码上有一个位不对就会引发错误。上一次实验中实现的“蓝屏”功能就帮到了我许多，因为最初的代码几乎总是引发一般保护错误（general protection error）或者页错误（page fault）的，“蓝屏”功能让我能立即定位到引发错误的代码，而不像实验三时那样从头开始一行一行运行过去（回想起来那真是十分痛苦的）极大帮助了我调试。

开启分页后，我本来打算在实验六的二状态进程模型上改进以适应分页，但发现由于之前的进程代码使用了过多汇编过程，许多地方硬编码了一些偏移量，要移除其中基于段式内存管理的那部分代码很困难，因此最后决定全部删去，参考Unix的代码重写。其实我还是很心痛的，毕竟实验六的进程模型是我通宵连续工作近20小时做出来的成果。

但分页真的是非常有用的。在上次实验报告的总结中，我分析了基于段式内存管理实现多进程时，为了分离不同进程的堆栈，导致ds和ss段地址不同，最终导致C语言函数传参出错的问题；并提出期望通过页式内存管理解决这个问题。本实验中分页确实解决了这一点，每一个用户程序进程都以虚拟地址0xc0001000开始运行，栈顶均是0xc0001000，在用户程序看来内存不再分段，“指哪打哪”，从而符合了C语言抽象机器模型（C abstract machine model）对于平坦内存的假设。我不再需要用我写的那个奇怪的基于模板和内联汇编的my_va_arg，也可以开开心心地在多进程的环境下写用户C程序了。

本次实验让我觉得收获最大的是：通过阅读Unix系统的代码，并在我的系统中加以实现，让我彻底搞懂了Unix系统中的进程生命周期和进程管理有关的函数的工作原理。Unix系统中进程这部分有很多让我觉得比较难搞懂的内容：

- 进入和离开利用中断时的压栈和出栈过程构造中断帧
- 利用特权级切换进行内核栈和用户栈的切换
- 手动创建的init时的内核栈的构造
- 调度器选择进程运行、进程又通过sched返回调度器的多进程调度过程
- 通过fork后用exec替换自身镜像的加载程序方式

但同时，这些内容大都也是设计巧妙，令人学到很多的。

最后，有了分页和基于它重新实现的进程机制，我实现了加载ELF这种广泛应用于主流操作系统的可执行文件。一直以来都让我好奇的问题：“普通文件是用程序打开，那么可执行程序到底是怎么打开的”，也终于得到了比实验二中“直接把二进制文件载入内存”更让我满意的回答。

## 七、参考文献

《x86汇编语言 从实模式到保护模式》，李忠、王晓波、余洁

《Linux内核完全剖析——基于0.12内核》，赵炯

《xv6 中文文档》https://legacy.gitbook.com/book/th0ar/xv6-chinese/details

xv6 OS https://github.com/mit-pdos/xv6-public

OS 67 https://github.com/SilverRainZ/OS67